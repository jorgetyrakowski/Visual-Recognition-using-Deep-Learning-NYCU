# Global Wheat Detection - YOLOv11m Pipeline ("Ultimo intento")

This repository contains the code for a comprehensive pipeline to address the Global Wheat Detection challenge, utilizing the YOLOv11m architecture and advanced techniques such as K-Fold cross-validation, Out-of-Fold (OOF) parameter optimization, pseudo-labeling, Test-Time Augmentation (TTA), and Weighted Boxes Fusion (WBF).

This pipeline achieved a private leaderboard score of **0.6873** and a public score of **0.7691**.

## Project Structure

```
├── train_all_folds.sh        # Shell script to train 5 K-Fold base YOLOv11m models
├── tools/                    # Utility scripts for data preparation
│   └── make_folds.py         # Creates K-Fold data splits and YAML config files
├── pipeline_scripts/         # Modular Python scripts for the main pipeline
│   ├── config_and_utils.py   # Central configuration and core utility functions (TTA, WBF, etc.)
│   ├── oof_evaluation.py     # OOF evaluation and final score threshold optimization
│   ├── pseudo_labeler.py     # Pseudo-label generation and PL dataset preparation
│   ├── train_pl_model.py     # Trains/fine-tunes a model on the PL-augmented dataset
│   └── final_submission_generator.py # Generates the final submission.csv
├── run_full_pipeline.sh      # Main orchestrator script to run the entire pipeline after K-Fold training
├── yolov11m.pt               # Base COCO pre-trained YOLOv11m model weights
├── fold0.yaml ... fold4.yaml # Dataset YAMLs for K-Fold training (generated by make_folds.py)
├── lists/                    # Contains train/val image lists for each fold (generated by make_folds.py)
│   ├── train_fold0.txt ...
│   └── val_fold0.txt ...
├── Report.md                 # Detailed project report (drafted with Cline)
└── README.md                 # This file
```

**Expected Kaggle Input Datasets:**
*   `global-wheat-detection`: The official competition dataset.
*   `gw11-weights`: A Kaggle dataset you create, containing:
    *   `best_fold0.pt`
    *   `best_fold1.pt`
    *   `best_fold2.pt`
    *   `best_fold3.pt`
    *   `best_fold4.pt` (These are the outputs of `train_all_folds.sh`)
    *   `yolov11m.pt` (The base COCO pre-trained YOLOv11m model)
*   `ultralytics-wheels`: A Kaggle dataset containing the `.whl` files for offline installation of `ultralytics`, `ultralytics_thop`, and `ensemble_boxes`.

## Dependencies & Setup

*   Python 3.8+
*   PyTorch
*   Ultralytics YOLO (version compatible with YOLOv11m, e.g., v8.3.139 as used in `inference.py`)
*   `ensemble-boxes`
*   Pandas, NumPy, OpenCV-Python, Scikit-learn, PyYAML, tqdm, Numba

These can be installed in a Kaggle Notebook environment using the pip install commands for the wheels, as shown at the beginning of the Python scripts in `pipeline_scripts/`. For local setup, create a virtual environment and install these packages. A `requirements.txt` could be generated from such an environment.

## Pipeline Execution Order

**Phase 1: Local Data Preparation and K-Fold Model Training**
*(These steps are typically run locally or on a persistent environment due to training time.)*

1.  **Prepare Initial Data:**
    *   Ensure the GWD competition data (`train.csv`, `train/` images, `test/` images, `sample_submission.csv`) is available (e.g., in a `data/` subdirectory relative to where `make_folds.py` is run, or update paths in scripts).
    *   The `tools/make_folds.py` script assumes that corresponding YOLO label files (`.txt`) for the training images either exist alongside the images or are findable by YOLOv8/v11 (e.g., in a parallel `../labels` directory). If not, you would first need to run a script like `tools/convert_gwd_to_yolo.py` (from your broader "Ultimo intento" folder, not explicitly part of this modular pipeline) to generate these initial YOLO labels from `train.csv`.
        *Example to generate initial labels (if needed, run from `Ultimo intento/`):*
        `python tools/convert_gwd_to_yolo.py --csv ../data/train.csv --img_dir ../data/train --out_labels ../data/train_labels_yolo`
        *(Adjust paths as per your actual raw data location. The `make_folds.py` script then needs `--images` to point to where images are, and `--labels` to where these generated .txt labels are, or it assumes labels are co-located/findable).*
        **Note:** The provided `tools/make_folds.py` in "Ultimo intento" *itself* handles the conversion of `train.csv` bounding boxes to YOLO format labels and copies images/labels into per-fold directories. So, an external `convert_gwd_to_yolo.py` run might not be needed if `make_folds.py` is used with the original `train.csv` and image directory.

2.  **Create K-Fold Splits and YAMLs:**
    *   Navigate to the `Ultimo intento/` directory.
    *   Run `tools/make_folds.py`. This script expects the original GWD `train.csv` and the directory of training images as input.
        ```bash
        python tools/make_folds.py \
            --images /path/to/gwd_dataset/train \
            --labels /path/to/gwd_dataset_yolo_labels \ # Or dir where labels will be relative to images
            --csv /path/to/gwd_dataset/train.csv \
            --folds 5
        ```
        *(Adjust paths. This creates `fold{k}.yaml` and `lists/` subdirectory in `Ultimo intento/`)*.
        **Correction based on `make_folds.py` logic:** The script copies images and generates labels into `Ultimo intento/folds/fold{k}/images/` and `labels/`. The YAMLs then point to these. So, the `--images` argument should be your *original* GWD train image directory. The `--labels` argument is somewhat vestigial in this specific `make_folds.py` as it re-generates labels.

3.  **Train K-Fold Base Models:**
    *   Ensure `yolov11m.pt` (base COCO pre-trained weights) is in the `Ultimo intento/` directory.
    *   Run the training script:
        ```bash
        bash train_all_folds.sh
        ```
    *   This will create 5 trained models in `Ultimo intento/gw11/gwd_fold0/weights/best.pt`, ..., `Ultimo intento/gw11/gwd_fold4/weights/best.pt`.
    *   **Action:** Copy these 5 `best.pt` files and the base `yolov11m.pt` into a new Kaggle Dataset (e.g., named "gw11-weights"). Update paths in `pipeline_scripts/config_and_utils.py` if your Kaggle dataset name differs.

**Phase 2: Kaggle Notebook Execution (using the modular pipeline)**
*(This is typically run in a Kaggle Notebook environment.)*

1.  **Setup Kaggle Notebook:**
    *   Upload your `Ultimo intento/pipeline_scripts/` directory (containing `config_and_utils.py`, `oof_evaluation.py`, etc.) as a utility script source or part of your notebook's codebase.
    *   Upload `Ultimo intento/run_full_pipeline.sh` as the main script to execute, or create a notebook that calls the Python scripts in order.
    *   Add your Kaggle datasets: `global-wheat-detection` and your "gw11-weights" (containing the 5 fold models and base `yolov11m.pt`).
    *   Add the `ultralytics-wheels` dataset.

2.  **Run the Full Pipeline:**
    *   If using the shell script orchestrator, execute it from the root of your Kaggle working directory (where `pipeline_scripts/` is a subdirectory):
        ```bash
        bash run_full_pipeline.sh
        ```
    *   This script will execute the Python modules in sequence:
        1.  `oof_evaluation.py`: Performs OOF evaluation and saves `optimized_oof_params.json` to `/kaggle/working/`.
        2.  `pseudo_labeler.py`: Reads optimized params, generates PLs, and prepares the PL dataset in `/kaggle/working/pseudo_labeled_data/`.
        3.  `train_pl_model.py`: Trains a new model on the PL dataset, saving it to `/kaggle/working/runs/detect/pseudo_labeled_wheat_model/weights/best.pt`.
        4.  `final_submission_generator.py`: Loads the PL-trained model and optimized params, generates `submission.csv` in `/kaggle/working/`.

## Expected Outputs
*   `optimized_oof_params.json`: Contains the optimized final score threshold.
*   `pseudo_labeled_data/`: Directory with the combined training data (GT + PLs) and `dataset_pl.yaml`.
*   `runs/detect/pseudo_labeled_wheat_model/weights/best.pt`: The final model trained with pseudo-labels.
*   `submission.csv`: The final submission file for the Kaggle competition.

This modular structure allows for easier debugging and understanding of each pipeline stage.
